services:
  t2v-transformers:
    image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    environment:
      ENABLE_CUDA: 0 # set to 1 to enable
      # NVIDIA_VISIBLE_DEVICES: all # enable if running with CUDA
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.24.11
    restart: on-failure:0
    ports:
    - "8080:8080"
    - "50051:50051"
    volumes:
      - weaviate_data:/var/lib/weaviate
    environment:
      QUERY_DEFAULTS_LIMIT: 100
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: text2vec-transformers
      ENABLE_MODULES: text2vec-transformers
      TRANSFORMERS_INFERENCE_API: http://t2v-transformers:8080
    depends_on:
      - t2v-transformers
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    depends_on:
      - weaviate
    volumes:
      - ollama_data:/root/.ollama
  ollama-webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    depends_on:
      - ollama
    ports:
      - 3000:8080
    environment:
      - "OLLAMA_API_BASE_URL=http://ollama:11434/api"
    restart: unless-stopped
  web:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["./start.sh"]
    environment:
      WEAVIATE_HOST_URL: "weaviate"
      OLLAMA_API_BASE_URL: "http://ollama:11434"
    volumes:
      - .:/app
    ports:
      - "8501:8501"
    depends_on:
      - ollama
volumes:
  weaviate_data:
  ollama_data: